# ãƒ‡ãƒ¼ã‚¿ç§»è¡Œæˆ¦ç•¥ï¼šæ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®æ–°ã‚¹ã‚­ãƒ¼ãƒç§»è¡Œ

## ğŸ¯ ç§»è¡Œã®ç›®æ¨™

**ãƒ¡ã‚¤ãƒ³ç›®æ¨™**:
- æ—¢å­˜ã®`practice_history`ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰æ–°ã—ã„æ­£è¦åŒ–ã•ã‚ŒãŸã‚¹ã‚­ãƒ¼ãƒã¸ã®ãƒ‡ãƒ¼ã‚¿ç§»è¡Œ
- ã‚¼ãƒ­ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ ã§ã®ç§»è¡Œå®Ÿç¾
- ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã®100%ä¿è¨¼
- Streamlit Cloudç’°å¢ƒã§ã®å±¥æ­´ç¶™ç¶šæ€§ç¢ºä¿

**æŠ€è¡“è¦ä»¶**:
- ãƒ‡ãƒ¼ã‚¿æå¤±: 0ä»¶
- ç§»è¡Œä¸­ã®ã‚µãƒ¼ãƒ“ã‚¹åœæ­¢: æœ€å¤§5åˆ†ä»¥å†…
- ç§»è¡Œå¾Œã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Š: 50%ä»¥ä¸Š
- ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚é–“: 2åˆ†ä»¥å†…

---

## ğŸ“Š ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿åˆ†æ

### 1. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®è©³ç´°åˆ†æ

```sql
-- ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿é‡åˆ†æ
SELECT 
    practice_type,
    COUNT(*) as record_count,
    MIN(practice_date) as earliest_date,
    MAX(practice_date) as latest_date,
    AVG(duration_seconds) as avg_duration
FROM practice_history 
GROUP BY practice_type
ORDER BY record_count DESC;

-- å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®è¤‡é›‘ã•åˆ†æ
SELECT 
    practice_type,
    jsonb_object_keys(inputs) as input_key,
    COUNT(*) as frequency
FROM practice_history, jsonb_object_keys(inputs) 
GROUP BY practice_type, jsonb_object_keys(inputs)
ORDER BY practice_type, frequency DESC;

-- ã‚¹ã‚³ã‚¢ãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ åˆ†æ
SELECT 
    practice_type,
    jsonb_object_keys(scores) as score_key,
    COUNT(*) as frequency,
    AVG((scores->>jsonb_object_keys(scores))::numeric) as avg_score
FROM practice_history, jsonb_object_keys(scores)
WHERE scores IS NOT NULL
GROUP BY practice_type, jsonb_object_keys(scores);
```

### 2. ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯

```sql
-- ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ã‚¯ã‚¨ãƒª
WITH data_quality AS (
    SELECT 
        id,
        session_id,
        practice_type,
        practice_date,
        CASE 
            WHEN inputs IS NULL OR inputs = '{}' THEN 'missing_inputs'
            WHEN scores IS NULL OR scores = '{}' THEN 'missing_scores'
            WHEN feedback IS NULL OR feedback = '' THEN 'missing_feedback'
            WHEN duration_seconds IS NULL THEN 'missing_duration'
            ELSE 'valid'
        END as data_status
    FROM practice_history
)
SELECT 
    data_status,
    COUNT(*) as record_count,
    ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage
FROM data_quality
GROUP BY data_status
ORDER BY record_count DESC;
```

---

## ğŸ”„ æ®µéšçš„ç§»è¡Œæˆ¦ç•¥

### ãƒ•ã‚§ãƒ¼ã‚º1: æº–å‚™æ®µéšï¼ˆ1-2æ—¥ï¼‰

#### 1.1 ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
```bash
# å®Œå…¨ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ä½œæˆ
pg_dump -h your-supabase-host -U postgres -d your-database \
  --data-only --inserts --table=practice_history \
  --table=user_sessions > backup_before_migration.sql

# Supabase DashboardçµŒç”±ã§ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç¢ºèª
```

#### 1.2 æ–°ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ã®ä½œæˆ
```sql
-- æ–°ã—ã„ãƒ†ãƒ¼ãƒ–ãƒ«ç¾¤ã‚’ä½œæˆï¼ˆæ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ä¸¦è¡Œï¼‰
-- technical_specifications.mdã®DDLã‚’å®Ÿè¡Œ

-- ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆç¢ºèª
SELECT table_name, table_rows 
FROM information_schema.tables 
WHERE table_schema = 'public' 
AND table_name IN ('users', 'practice_categories', 'practice_types', 
                   'practice_sessions', 'practice_inputs', 'practice_scores');
```

#### 1.3 åŸºç¤ãƒ‡ãƒ¼ã‚¿ã®æŠ•å…¥
```sql
-- ã‚«ãƒ†ã‚´ãƒªã¨ã‚¿ã‚¤ãƒ—ã®ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿æŠ•å…¥
INSERT INTO practice_categories (category_name, display_name, icon, color, sort_order) VALUES
('exam_prep', 'æ¡ç”¨è©¦é¨“ç³»', 'ğŸ“„', '#667eea', 1),
('reading', 'è‹±èªèª­è§£ç³»', 'ğŸ“–', '#3b82f6', 2),
('writing', 'è¨˜è¿°ç³»', 'âœï¸', '#8b5cf6', 3),
('interview', 'é¢æ¥ç³»', 'ğŸ—£ï¸', '#f59e0b', 4),
('research', 'è«–æ–‡ç ”ç©¶ç³»', 'ğŸ”¬', '#22c55e', 5);

-- ç·´ç¿’ã‚¿ã‚¤ãƒ—ã®ãƒãƒƒãƒ”ãƒ³ã‚°å®šç¾©
WITH type_mapping AS (
    SELECT unnest(ARRAY[
        ('æ¡ç”¨è©¦é¨“', 1, 'standard_exam', 'æ¨™æº–æ¡ç”¨è©¦é¨“'),
        ('éå»å•ã‚¹ã‚¿ã‚¤ãƒ«æ¡ç”¨è©¦é¨“', 1, 'past_exam_standard', 'éå»å•æ¡ç”¨è©¦é¨“ï¼ˆæ¨™æº–ï¼‰'),
        ('éå»å•ã‚¹ã‚¿ã‚¤ãƒ«æ¡ç”¨è©¦é¨“ - Letterå½¢å¼ï¼ˆç¿»è¨³ + æ„è¦‹ï¼‰', 1, 'past_exam_letter', 'éå»å•æ¡ç”¨è©¦é¨“ï¼ˆLetterï¼‰'),
        ('éå»å•ã‚¹ã‚¿ã‚¤ãƒ«æ¡ç”¨è©¦é¨“ - è«–æ–‡ã‚³ãƒ¡ãƒ³ãƒˆå½¢å¼ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆç¿»è¨³ + æ„è¦‹ï¼‰', 1, 'past_exam_comment', 'éå»å•æ¡ç”¨è©¦é¨“ï¼ˆCommentï¼‰'),
        ('è‹±èªèª­è§£', 2, 'standard_reading', 'æ¨™æº–è‹±èªèª­è§£'),
        ('åŒ»å­¦éƒ¨æ¡ç”¨è©¦é¨“ è‡ªç”±è¨˜è¿°', 3, 'free_writing', 'è‡ªç”±è¨˜è¿°'),
        ('å°è«–æ–‡å¯¾ç­–', 3, 'essay_writing', 'å°è«–æ–‡å¯¾ç­–'),
        ('é¢æ¥å¯¾ç­–(å˜ç™º)', 4, 'interview_single', 'å˜ç™ºé¢æ¥'),
        ('é¢æ¥å¯¾ç­–(ã‚»ãƒƒã‚·ãƒ§ãƒ³)', 4, 'interview_session', 'ã‚»ãƒƒã‚·ãƒ§ãƒ³é¢æ¥'),
        ('ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç”Ÿæˆ', 5, 'keyword_generation', 'ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç”Ÿæˆ'),
        ('è«–æ–‡æ¤œç´¢', 5, 'paper_search', 'è«–æ–‡æ¤œç´¢')
    ]) AS t(old_name, category_id, type_name, display_name)
)
INSERT INTO practice_types (category_id, type_name, display_name, input_schema, score_schema)
SELECT 
    category_id,
    type_name,
    display_name,
    '{"fields": ["default"]}' as input_schema,
    '{"categories": ["ç·åˆè©•ä¾¡"]}' as score_schema
FROM type_mapping;
```

### ãƒ•ã‚§ãƒ¼ã‚º2: ãƒ‡ãƒ¼ã‚¿å¤‰æ›ãƒ»ç§»è¡Œï¼ˆ2-3æ—¥ï¼‰

#### 2.1 ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚»ãƒƒã‚·ãƒ§ãƒ³ç§»è¡Œ
```sql
-- æ—¢å­˜ã®session_idã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ä½œæˆ
INSERT INTO users (user_id, display_name, created_at)
SELECT DISTINCT
    session_id::uuid as user_id,
    'Legacy User ' || substr(session_id, 1, 8) as display_name,
    MIN(practice_date) as created_at
FROM practice_history
WHERE session_id IS NOT NULL
GROUP BY session_id;
```

#### 2.2 ç·´ç¿’ã‚»ãƒƒã‚·ãƒ§ãƒ³ç§»è¡Œ
```sql
-- practice_history â†’ practice_sessions
INSERT INTO practice_sessions (
    session_id, user_id, practice_type_id, theme, 
    start_time, end_time, duration_seconds, status, created_at
)
SELECT 
    gen_random_uuid() as session_id,
    ph.session_id::uuid as user_id,
    pt.practice_type_id,
    COALESCE(ph.inputs->>'theme', ph.inputs->>'category', '') as theme,
    ph.practice_date as start_time,
    ph.practice_date + INTERVAL '1 second' * COALESCE(ph.duration_seconds, 0) as end_time,
    ph.duration_seconds,
    'completed' as status,
    ph.created_at
FROM practice_history ph
JOIN practice_types pt ON pt.type_name = (
    CASE 
        WHEN ph.practice_type = 'æ¡ç”¨è©¦é¨“' THEN 'standard_exam'
        WHEN ph.practice_type = 'éå»å•ã‚¹ã‚¿ã‚¤ãƒ«æ¡ç”¨è©¦é¨“ - Letterå½¢å¼ï¼ˆç¿»è¨³ + æ„è¦‹ï¼‰' THEN 'past_exam_letter'
        WHEN ph.practice_type = 'åŒ»å­¦éƒ¨æ¡ç”¨è©¦é¨“ è‡ªç”±è¨˜è¿°' THEN 'free_writing'
        -- ä»–ã®ãƒãƒƒãƒ”ãƒ³ã‚°...
        ELSE 'standard_exam'
    END
)
WHERE ph.session_id IS NOT NULL;
```

#### 2.3 å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–ç§»è¡Œ
```sql
-- è¤‡é›‘ãªJSONB inputsã‚’æ­£è¦åŒ–
WITH input_extraction AS (
    SELECT 
        ps.session_id,
        ph.inputs,
        ph.practice_type,
        -- å„ç·´ç¿’ã‚¿ã‚¤ãƒ—åˆ¥ã®å…¥åŠ›ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
        CASE 
            WHEN ph.practice_type LIKE '%Letterå½¢å¼%' THEN 
                jsonb_build_array(
                    jsonb_build_object('type', 'original_paper', 'content', ph.inputs->>'original_paper'),
                    jsonb_build_object('type', 'translation', 'content', ph.inputs->>'translation'),
                    jsonb_build_object('type', 'opinion', 'content', ph.inputs->>'opinion')
                )
            WHEN ph.practice_type = 'åŒ»å­¦éƒ¨æ¡ç”¨è©¦é¨“ è‡ªç”±è¨˜è¿°' THEN
                jsonb_build_array(
                    jsonb_build_object('type', 'question', 'content', ph.inputs->>'question'),
                    jsonb_build_object('type', 'answer', 'content', ph.inputs->>'answer')
                )
            ELSE 
                jsonb_build_array(
                    jsonb_build_object('type', 'content', 'content', ph.inputs::text)
                )
        END as extracted_inputs
    FROM practice_sessions ps
    JOIN practice_history ph ON ph.session_id::uuid = ps.user_id
),
input_normalization AS (
    SELECT 
        session_id,
        (jsonb_array_elements(extracted_inputs)->>'type') as input_type,
        (jsonb_array_elements(extracted_inputs)->>'content') as content,
        row_number() OVER (PARTITION BY session_id ORDER BY ordinality) as input_order
    FROM input_extraction, jsonb_array_elements(extracted_inputs) WITH ORDINALITY
    WHERE jsonb_array_elements(extracted_inputs)->>'content' IS NOT NULL
    AND jsonb_array_elements(extracted_inputs)->>'content' != ''
)
INSERT INTO practice_inputs (session_id, input_type, content, input_order, word_count)
SELECT 
    session_id,
    input_type,
    content,
    input_order,
    LENGTH(content) as word_count
FROM input_normalization;
```

#### 2.4 ã‚¹ã‚³ã‚¢ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–ç§»è¡Œ
```sql
-- JSONBã‚¹ã‚³ã‚¢ã®æ­£è¦åŒ–
WITH score_extraction AS (
    SELECT 
        ps.session_id,
        ph.scores,
        score_key,
        (ph.scores->>score_key)::numeric as score_value
    FROM practice_sessions ps
    JOIN practice_history ph ON ph.session_id::uuid = ps.user_id,
         jsonb_object_keys(ph.scores) as score_key
    WHERE ph.scores IS NOT NULL 
    AND ph.scores != '{}'
    AND (ph.scores->>score_key) ~ '^[0-9]+\.?[0-9]*$'
)
INSERT INTO practice_scores (session_id, score_category, score_value, max_score)
SELECT 
    session_id,
    score_key as score_category,
    score_value,
    CASE 
        WHEN score_value <= 10 THEN 10.00
        WHEN score_value <= 100 THEN 100.00
        ELSE 10.00
    END as max_score
FROM score_extraction;
```

#### 2.5 ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ç§»è¡Œ
```sql
-- ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã®ç§»è¡Œ
INSERT INTO practice_feedback (session_id, feedback_content, feedback_type)
SELECT 
    ps.session_id,
    ph.feedback,
    CASE 
        WHEN ph.feedback LIKE '%ã‚¨ãƒ©ãƒ¼%' OR ph.feedback LIKE '%UNAVAILABLE%' THEN 'error'
        ELSE 'general'
    END as feedback_type
FROM practice_sessions ps
JOIN practice_history ph ON ph.session_id::uuid = ps.user_id
WHERE ph.feedback IS NOT NULL 
AND ph.feedback != '';
```

### ãƒ•ã‚§ãƒ¼ã‚º3: æ¤œè¨¼ãƒ»åˆ‡ã‚Šæ›¿ãˆï¼ˆ1æ—¥ï¼‰

#### 3.1 ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§æ¤œè¨¼
```sql
-- ç§»è¡Œãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
WITH migration_check AS (
    SELECT 
        'practice_history' as source_table,
        COUNT(*) as record_count
    FROM practice_history
    
    UNION ALL
    
    SELECT 
        'practice_sessions' as source_table,
        COUNT(*) as record_count
    FROM practice_sessions
    
    UNION ALL
    
    SELECT 
        'practice_inputs' as source_table,
        COUNT(*) as record_count
    FROM practice_inputs
    
    UNION ALL
    
    SELECT 
        'practice_scores' as source_table,
        COUNT(*) as record_count  
    FROM practice_scores
)
SELECT * FROM migration_check;

-- è©³ç´°æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
SELECT 
    ps.session_id,
    ps.user_id,
    COUNT(pi.input_id) as input_count,
    COUNT(psc.score_id) as score_count,
    COUNT(pf.feedback_id) as feedback_count
FROM practice_sessions ps
LEFT JOIN practice_inputs pi ON ps.session_id = pi.session_id
LEFT JOIN practice_scores psc ON ps.session_id = psc.session_id
LEFT JOIN practice_feedback pf ON ps.session_id = pf.session_id
GROUP BY ps.session_id, ps.user_id
HAVING COUNT(pi.input_id) = 0 OR COUNT(psc.score_id) = 0
LIMIT 10;
```

#### 3.2 ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
```sql
-- ã‚¯ã‚¨ãƒªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ
EXPLAIN (ANALYZE, BUFFERS) 
SELECT * FROM practice_history 
WHERE session_id = 'test-session-id' 
ORDER BY practice_date DESC 
LIMIT 10;

EXPLAIN (ANALYZE, BUFFERS)
SELECT 
    ps.*,
    pt.display_name,
    array_agg(pi.content) as inputs,
    avg(psc.score_percentage) as avg_score
FROM practice_sessions ps
JOIN practice_types pt ON ps.practice_type_id = pt.practice_type_id
LEFT JOIN practice_inputs pi ON ps.session_id = pi.session_id
LEFT JOIN practice_scores psc ON ps.session_id = psc.session_id
WHERE ps.user_id = 'test-user-id'::uuid
GROUP BY ps.session_id, pt.display_name
ORDER BY ps.created_at DESC 
LIMIT 10;
```

#### 3.3 ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³åˆ‡ã‚Šæ›¿ãˆ
```python
# æ–°ã—ã„DatabaseManagerã¸ã®æ®µéšçš„åˆ‡ã‚Šæ›¿ãˆ
class MigrationDatabaseManager:
    def __init__(self, use_new_schema=False):
        self.use_new_schema = use_new_schema
        self.old_manager = OldDatabaseManager()
        self.new_manager = NewDatabaseManager() if use_new_schema else None
    
    def load_practice_history(self, user_id, practice_type=None):
        if self.use_new_schema and self.new_manager:
            try:
                return self.new_manager.load_practice_history(user_id, practice_type)
            except Exception as e:
                logger.error(f"New schema failed, falling back: {e}")
                return self.old_manager.load_practice_history(user_id, practice_type)
        else:
            return self.old_manager.load_practice_history(user_id, practice_type)
```

---

## ğŸ”§ ç§»è¡Œãƒ„ãƒ¼ãƒ«ãƒ»ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

### 1. ç§»è¡Œå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
```python
#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç§»è¡Œå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""
import os
import sys
import logging
from datetime import datetime
import psycopg2
from supabase import create_client

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DatabaseMigrator:
    def __init__(self):
        self.supabase = create_client(
            os.getenv('SUPABASE_URL'),
            os.getenv('SUPABASE_ANON_KEY')
        )
        
    def execute_migration(self):
        """ç§»è¡Œã‚’å®Ÿè¡Œ"""
        try:
            logger.info("Starting database migration...")
            
            # Phase 1: Backup
            self.create_backup()
            
            # Phase 2: Create new schema
            self.create_new_schema()
            
            # Phase 3: Migrate data
            self.migrate_users()
            self.migrate_sessions()
            self.migrate_inputs()
            self.migrate_scores()
            self.migrate_feedback()
            
            # Phase 4: Validate
            self.validate_migration()
            
            logger.info("Migration completed successfully!")
            
        except Exception as e:
            logger.error(f"Migration failed: {e}")
            self.rollback()
            raise
    
    def create_backup(self):
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_file = f"migration_backup_{timestamp}.sql"
        
        # Supabase REST APIã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        practice_history = self.supabase.table('practice_history').select('*').execute()
        user_sessions = self.supabase.table('user_sessions').select('*').execute()
        
        with open(backup_file, 'w') as f:
            f.write(f"-- Migration backup created at {datetime.now()}\n")
            f.write(f"-- Practice history records: {len(practice_history.data)}\n")
            f.write(f"-- User sessions: {len(user_sessions.data)}\n")
        
        logger.info(f"Backup created: {backup_file}")
    
    def migrate_users(self):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ç§»è¡Œ"""
        logger.info("Migrating users...")
        
        # æ—¢å­˜ã®session_idã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’ä½œæˆ
        existing_sessions = self.supabase.table('user_sessions').select('*').execute()
        
        for session in existing_sessions.data:
            user_data = {
                'user_id': session['session_id'],
                'display_name': f"User {session['session_id'][:8]}",
                'created_at': session['created_at'],
                'last_active': session['last_active']
            }
            
            try:
                self.supabase.table('users').insert(user_data).execute()
            except Exception as e:
                logger.warning(f"Failed to insert user {session['session_id']}: {e}")
    
    def validate_migration(self):
        """ç§»è¡Œæ¤œè¨¼"""
        logger.info("Validating migration...")
        
        # ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ãƒã‚§ãƒƒã‚¯
        old_count = len(self.supabase.table('practice_history').select('id').execute().data)
        new_count = len(self.supabase.table('practice_sessions').select('session_id').execute().data)
        
        if old_count != new_count:
            raise Exception(f"Record count mismatch: old={old_count}, new={new_count}")
        
        logger.info(f"Validation passed: {new_count} records migrated")
    
    def rollback(self):
        """ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œ"""
        logger.warning("Executing rollback...")
        # æ–°ã—ã„ãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤
        tables = ['practice_feedback', 'practice_scores', 'practice_inputs', 
                 'practice_sessions', 'users', 'practice_types', 'practice_categories']
        
        for table in tables:
            try:
                self.supabase.table(table).delete().neq('created_at', '1970-01-01').execute()
                logger.info(f"Cleared table: {table}")
            except Exception as e:
                logger.error(f"Failed to clear {table}: {e}")

if __name__ == "__main__":
    migrator = DatabaseMigrator()
    migrator.execute_migration()
```

### 2. ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
```python
#!/usr/bin/env python3
"""
ç§»è¡Œãƒ‡ãƒ¼ã‚¿æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

class MigrationValidator:
    def __init__(self, supabase_client):
        self.supabase = supabase_client
    
    def run_full_validation(self):
        """å®Œå…¨ãªæ¤œè¨¼ã‚’å®Ÿè¡Œ"""
        results = {
            'record_counts': self.validate_record_counts(),
            'data_integrity': self.validate_data_integrity(),
            'performance': self.validate_performance(),
            'user_migration': self.validate_user_migration()
        }
        
        return results
    
    def validate_record_counts(self):
        """ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã®æ¤œè¨¼"""
        counts = {}
        
        # æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«
        counts['practice_history'] = len(
            self.supabase.table('practice_history').select('id').execute().data
        )
        
        # æ–°ãƒ†ãƒ¼ãƒ–ãƒ«
        counts['practice_sessions'] = len(
            self.supabase.table('practice_sessions').select('session_id').execute().data
        )
        counts['practice_inputs'] = len(
            self.supabase.table('practice_inputs').select('input_id').execute().data
        )
        counts['practice_scores'] = len(
            self.supabase.table('practice_scores').select('score_id').execute().data
        )
        
        return counts
    
    def validate_data_integrity(self):
        """ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã®æ¤œè¨¼"""
        issues = []
        
        # å¤–éƒ¨ã‚­ãƒ¼åˆ¶ç´„ãƒã‚§ãƒƒã‚¯
        orphaned_inputs = self.supabase.rpc('check_orphaned_inputs').execute()
        if orphaned_inputs.data:
            issues.append(f"Orphaned inputs: {len(orphaned_inputs.data)}")
        
        return issues
    
    def validate_user_migration(self):
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ç§»è¡Œã®æ¤œè¨¼"""
        # session_id -> user_id ã®å¯¾å¿œç¢ºèª
        user_sessions = self.supabase.table('user_sessions').select('session_id').execute()
        users = self.supabase.table('users').select('user_id').execute()
        
        session_ids = {s['session_id'] for s in user_sessions.data}
        user_ids = {u['user_id'] for u in users.data}
        
        missing_users = session_ids - user_ids
        extra_users = user_ids - session_ids
        
        return {
            'missing_users': list(missing_users),
            'extra_users': list(extra_users),
            'migration_success_rate': 1.0 - (len(missing_users) / len(session_ids))
        }
```

---

## ğŸš¨ ãƒªã‚¹ã‚¯å¯¾ç­–ãƒ»ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨ˆç”»

### 1. ãƒªã‚¹ã‚¯ç®¡ç†ãƒãƒˆãƒªãƒƒã‚¯ã‚¹

| ãƒªã‚¹ã‚¯ | å½±éŸ¿åº¦ | ç™ºç”Ÿç¢ºç‡ | å¯¾ç­– | ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ‰‹é † |
|--------|--------|----------|------|------------------|
| ãƒ‡ãƒ¼ã‚¿æå¤± | é«˜ | ä½ | å®Œå…¨ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ— + æ®µéšçš„ç§»è¡Œ | ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰å¾©å…ƒ |
| é•·æ™‚é–“ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ  | ä¸­ | ä¸­ | Blue-Green ãƒ‡ãƒ—ãƒ­ã‚¤ | æ—§ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«åˆ‡ã‚Šæ›¿ãˆ |
| ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ä½ä¸‹ | ä¸­ | ä½ | äº‹å‰ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ | ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æœ€é©åŒ– |
| ä¸å®Œå…¨ãªç§»è¡Œ | é«˜ | ä¸­ | è©³ç´°æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ | æ®µéšçš„ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ |

### 2. ç·Šæ€¥æ™‚ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ‰‹é †

```sql
-- ç·Šæ€¥ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ SQL
-- Step 1: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ—§ã‚¹ã‚­ãƒ¼ãƒã«åˆ‡ã‚Šæ›¿ãˆï¼ˆã‚³ãƒ¼ãƒ‰å¤‰æ›´ï¼‰

-- Step 2: æ–°ãƒ†ãƒ¼ãƒ–ãƒ«ã®ä¸€æ™‚ç„¡åŠ¹åŒ–
ALTER TABLE practice_sessions RENAME TO practice_sessions_backup;
ALTER TABLE practice_inputs RENAME TO practice_inputs_backup;
ALTER TABLE practice_scores RENAME TO practice_scores_backup;
ALTER TABLE practice_feedback RENAME TO practice_feedback_backup;

-- Step 3: æ—¢å­˜ãƒ†ãƒ¼ãƒ–ãƒ«ã®å¾©å…ƒï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‹ã‚‰ï¼‰
-- \i migration_backup_YYYYMMDD_HHMMSS.sql

-- Step 4: æ¤œè¨¼
SELECT COUNT(*) FROM practice_history;
SELECT COUNT(*) FROM user_sessions;
```

### 3. æ®µéšçš„ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯

```python
class RollbackManager:
    def __init__(self, supabase_client):
        self.supabase = supabase_client
        self.rollback_steps = [
            self.rollback_feedback,
            self.rollback_scores,
            self.rollback_inputs,
            self.rollback_sessions,
            self.rollback_users,
            self.rollback_master_data
        ]
    
    def execute_rollback(self, target_step=None):
        """æ®µéšçš„ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å®Ÿè¡Œ"""
        for i, step in enumerate(self.rollback_steps):
            if target_step and i >= target_step:
                break
            try:
                step()
                logger.info(f"Rollback step {i+1} completed")
            except Exception as e:
                logger.error(f"Rollback step {i+1} failed: {e}")
                raise
    
    def rollback_feedback(self):
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        self.supabase.table('practice_feedback').delete().neq('created_at', '1970-01-01').execute()
    
    def rollback_scores(self):
        """ã‚¹ã‚³ã‚¢ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        self.supabase.table('practice_scores').delete().neq('created_at', '1970-01-01').execute()
    
    # ä»–ã®ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ‰‹é †...
```

---

## ğŸ“Š ç§»è¡Œå®Œäº†å¾Œã®æ¤œè¨¼é …ç›®

### 1. æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
```python
def test_post_migration_functionality():
    """ç§»è¡Œå¾Œã®æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ"""
    test_cases = [
        'test_user_history_retrieval',
        'test_practice_session_creation',
        'test_score_calculation',
        'test_statistics_generation',
        'test_cross_device_consistency'
    ]
    
    results = {}
    for test_case in test_cases:
        try:
            result = globals()[test_case]()
            results[test_case] = {'status': 'PASS', 'result': result}
        except Exception as e:
            results[test_case] = {'status': 'FAIL', 'error': str(e)}
    
    return results

def test_user_history_retrieval():
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼å±¥æ­´å–å¾—ãƒ†ã‚¹ãƒˆ"""
    # ãƒ†ã‚¹ãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã§ã®å±¥æ­´å–å¾—
    db = NewDatabaseManager()
    history = db.get_user_practice_history('test-user-id')
    assert len(history) > 0, "å±¥æ­´ãŒå–å¾—ã§ããªã„"
    assert all('session_id' in item for item in history), "ã‚»ãƒƒã‚·ãƒ§ãƒ³IDãŒä¸æ­£"
    return len(history)
```

### 2. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ
```python
import time
import statistics

def performance_comparison_test():
    """ç§»è¡Œå‰å¾Œã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ"""
    old_db = OldDatabaseManager()
    new_db = NewDatabaseManager()
    
    test_queries = [
        'get_user_history',
        'get_practice_statistics',
        'get_score_trends'
    ]
    
    results = {}
    for query in test_queries:
        old_times = []
        new_times = []
        
        # 10å›å®Ÿè¡Œã—ã¦å¹³å‡ã‚’è¨ˆç®—
        for _ in range(10):
            # æ—§ã‚·ã‚¹ãƒ†ãƒ 
            start = time.time()
            getattr(old_db, query)('test-user-id')
            old_times.append(time.time() - start)
            
            # æ–°ã‚·ã‚¹ãƒ†ãƒ 
            start = time.time()
            getattr(new_db, query)('test-user-id')
            new_times.append(time.time() - start)
        
        results[query] = {
            'old_avg': statistics.mean(old_times),
            'new_avg': statistics.mean(new_times),
            'improvement': 1 - (statistics.mean(new_times) / statistics.mean(old_times))
        }
    
    return results
```

---

## âœ… ç§»è¡Œå®Œäº†ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

### äº‹å‰æº–å‚™
- [ ] å®Œå…¨ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã®ä½œæˆ
- [ ] ç§»è¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
- [ ] ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯æ‰‹é †ã®ç¢ºèª
- [ ] é–¢ä¿‚è€…ã¸ã®äº‹å‰é€šçŸ¥

### ç§»è¡Œå®Ÿè¡Œ
- [ ] æ–°ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ ã®ä½œæˆ
- [ ] ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®æŠ•å…¥
- [ ] ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ç§»è¡Œ
- [ ] ç·´ç¿’ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ç§»è¡Œ
- [ ] å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–ç§»è¡Œ
- [ ] ã‚¹ã‚³ã‚¢ãƒ‡ãƒ¼ã‚¿ã®ç§»è¡Œ
- [ ] ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã®ç§»è¡Œ

### æ¤œè¨¼ãƒ»åˆ‡ã‚Šæ›¿ãˆ
- [ ] ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã®æ¤œè¨¼
- [ ] ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
- [ ] æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
- [ ] ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆ‡ã‚Šæ›¿ãˆ
- [ ] Streamlit Cloudç’°å¢ƒã§ã®å‹•ä½œç¢ºèª

### äº‹å¾Œå¯¾å¿œ
- [ ] ç§»è¡Œçµæœãƒ¬ãƒãƒ¼ãƒˆã®ä½œæˆ
- [ ] ç›£è¦–ä½“åˆ¶ã®ç¢ºç«‹
- [ ] ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®ç§»è¡Œå®Œäº†é€šçŸ¥
- [ ] æ—§ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–/å‰Šé™¤
- [ ] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ›´æ–°

---

## ğŸ“‹ ç§»è¡Œã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆè©³ç´°ï¼‰

| æ™‚é–“ | ä½œæ¥­å†…å®¹ | æ‹…å½“ | æ‰€è¦æ™‚é–“ | ãƒªã‚¹ã‚¯ |
|------|----------|------|----------|--------|
| Day 1 AM | ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ | Tech | 2h | ä½ |
| Day 1 PM | æ–°ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ | Tech | 4h | ä½ |
| Day 2 AM | ãƒ¦ãƒ¼ã‚¶ãƒ¼ç§»è¡Œ | Tech | 3h | ä¸­ |
| Day 2 PM | ã‚»ãƒƒã‚·ãƒ§ãƒ³ç§»è¡Œ | Tech | 4h | ä¸­ |
| Day 3 AM | å…¥åŠ›/ã‚¹ã‚³ã‚¢ç§»è¡Œ | Tech | 4h | é«˜ |
| Day 3 PM | æ¤œè¨¼ãƒ»ãƒ†ã‚¹ãƒˆ | Tech | 3h | ä¸­ |
| Day 4 AM | ã‚¢ãƒ—ãƒªåˆ‡ã‚Šæ›¿ãˆ | Tech | 2h | é«˜ |
| Day 4 PM | æœ¬ç•ªç¢ºèª | All | 2h | ä¸­ |

**æ³¨æ„**: å„ä½œæ¥­ã«ã¯30åˆ†ã®ãƒãƒƒãƒ•ã‚¡æ™‚é–“ã‚’å«ã‚€ 